{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361bc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be575d",
   "metadata": {},
   "source": [
    "### Import annual feature data; Extract X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943718fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_features = pd.read_csv('annual_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16114f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = annual_features.drop(['response'], axis = 1)\n",
    "y = annual_features['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc057615",
   "metadata": {},
   "source": [
    "### Split train and test set and compute the completeness of X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33a5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of non-zero elements in X_train is: 63.66 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2/3, random_state=1147)\n",
    "\n",
    "print(\"The percentage of non-zero elements in X_train is: {:.2f} %\".format(np.count_nonzero(X_train)/ X_train.size * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5640014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = X_train.columns\n",
    "completeness = []\n",
    "for i in range(len(X_train.columns)):\n",
    "    completeness.append(np.count_nonzero(X_train.iloc[:,i]) /len(X_train.iloc[:,i]) * 100)\n",
    "    # Since the dataset doesnot contain NaN value, count completeness using the non-zero counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefcbf69",
   "metadata": {},
   "source": [
    "### Standardize the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d9c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22414711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(random_state=1148)\n",
    "# params = {'n_estimators':[20,50,200,500],'max_features':[\"auto\", \"sqrt\", \"log2\"]}\n",
    "# # Gridsearch best hyperparameters\n",
    "# grid = GridSearchCV(rf, param_grid=params, scoring='accuracy', cv=5)\n",
    "# grid.fit(X_train, y_train)\n",
    "# grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4f4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 500, max_features=\"auto\", random_state=1148)\n",
    "rf.fit(X_train, y_train) \n",
    "importances = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c6d41",
   "metadata": {},
   "source": [
    "Using GridSearchCV tuning the best parameters of random forest classifier, and fit the model get the feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823ed53",
   "metadata": {},
   "source": [
    "### Create the completeness-feature importance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e146af6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014_ann_txn_amt_sum</th>\n",
       "      <td>0.048159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013_ann_txn_amt_sum</th>\n",
       "      <td>0.036371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014_ann_txn_amt_cnt</th>\n",
       "      <td>0.029129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013_ann_txn_amt_ave</th>\n",
       "      <td>0.027130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014_ann_txn_amt_ave</th>\n",
       "      <td>0.026992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012_ann_txn_amt_ave</th>\n",
       "      <td>0.024073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013_ann_txn_amt_sem</th>\n",
       "      <td>0.023371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012_ann_txn_amt_sem</th>\n",
       "      <td>0.023228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014_ann_txn_amt_max</th>\n",
       "      <td>0.021767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013_ann_txn_amt_var</th>\n",
       "      <td>0.021751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature Importance\n",
       "2014_ann_txn_amt_sum            0.048159\n",
       "2013_ann_txn_amt_sum            0.036371\n",
       "2014_ann_txn_amt_cnt            0.029129\n",
       "2013_ann_txn_amt_ave            0.027130\n",
       "2014_ann_txn_amt_ave            0.026992\n",
       "2012_ann_txn_amt_ave            0.024073\n",
       "2013_ann_txn_amt_sem            0.023371\n",
       "2012_ann_txn_amt_sem            0.023228\n",
       "2014_ann_txn_amt_max            0.021767\n",
       "2013_ann_txn_amt_var            0.021751"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame(importances, index=colnames, columns=['Feature Importance'])\n",
    "feature_importance = feature_importance.sort_values(by=['Feature Importance'], ascending=False)\n",
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a8c301",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_completeness' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kd/wjk9nfp95hj3fdh4_msxfl8h0000gn/T/ipykernel_1047/1307534732.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCompleteness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_completeness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Completeness'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mCompleteness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompleteness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Completeness'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCompleteness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_completeness' is not defined"
     ]
    }
   ],
   "source": [
    "Completeness = pd.DataFrame(log_completeness, index=colnames, columns=['Completeness'])\n",
    "Completeness = Completeness.sort_values(by=['Completeness'], ascending=False)\n",
    "Completeness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "log_completeness = np.log(np.array(completeness))\n",
    "plt.plot(log_completeness, importances, 'o', color='black')\n",
    "plt.xlabel('Completeness(%)-log scale')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Completeness vs Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6163542",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7b924",
   "metadata": {},
   "source": [
    "There is no missing value in this dataset, as in previous assignments the missing values are filled with 0. This the completeness is computed by count the non-zero element in each column of the training set.   \n",
    "The completeness-feature importance graph above show s a strong correlation between completeness of training set in log-scale and feature importance. As the completeness of feature increases, the feature importance grows from 0.00 to 0.05. Features with completeness greater than 4.5 (log scale) are have higher feature importance, the highest would be approximate 0.05.  \n",
    "From the graph, higher completeness tends to have higher feature importance. Thus, the data should be redesigned with the features with higher completeness, which means there should be less missing values in the data. When data collecting, all useful information should be collected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
